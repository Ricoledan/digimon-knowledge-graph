{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07: Predictive Modeling & Machine Learning\n",
    "\n",
    "**Objective**: Build predictive models for Digimon characteristics using machine learning.\n",
    "\n",
    "This notebook explores:\n",
    "- Classification tasks (predict Type, Attribute, evolution likelihood)\n",
    "- Feature engineering from graph properties\n",
    "- Model training and evaluation\n",
    "- Feature importance analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:03:10.999817Z",
     "start_time": "2025-08-12T02:03:10.167211Z"
    }
   },
   "source": "# Standard imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nfrom collections import Counter, defaultdict\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine learning imports\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Try to import XGBoost, but make it optional\ntry:\n    import xgboost as xgb\n    HAS_XGBOOST = True\n    print(\"XGBoost is available\")\nexcept Exception as e:  # Catch any exception, including XGBoostError\n    print(f\"Warning: XGBoost not available ({type(e).__name__}). Will use GradientBoosting as fallback.\")\n    HAS_XGBOOST = False\n    xgb = None  # Define xgb as None to avoid NameError later\n\n# Try to import SHAP, but make it optional\ntry:\n    import shap\n    HAS_SHAP = True\n    print(\"SHAP is available\")\nexcept ImportError:\n    print(\"Warning: SHAP not available. Will skip SHAP analysis.\")\n    HAS_SHAP = False\n    shap = None\n\n# Custom utilities\nfrom utils import (\n    Neo4jConnector,\n    save_figure,\n    TYPE_COLORS, LEVEL_COLORS, ATTRIBUTE_COLORS\n)\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Notebook configuration\nnotebook_name = \"07_predictive_modeling\"\n\nprint(\"\\nEnvironment setup complete!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:03:13.950977Z",
     "start_time": "2025-08-12T02:03:13.753896Z"
    }
   },
   "source": [
    "# Connect to database\n",
    "conn = Neo4jConnector()\n",
    "print(\"Connected to Neo4j database\")\n",
    "\n",
    "# Load all data\n",
    "digimon_df = conn.get_all_digimon()\n",
    "evolution_data = conn.get_evolution_chains()\n",
    "moves_df = conn.get_digimon_moves()\n",
    "\n",
    "print(f\"\\nLoaded data:\")\n",
    "print(f\"  - {len(digimon_df)} Digimon\")\n",
    "print(f\"  - {len(evolution_data)} evolution relationships\")\n",
    "print(f\"  - {len(moves_df)} move relationships\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j database\n",
      "\n",
      "Loaded data:\n",
      "  - 1258 Digimon\n",
      "  - 3746 evolution relationships\n",
      "  - 2433 move relationships\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:03:15.206493Z",
     "start_time": "2025-08-12T02:03:14.985375Z"
    }
   },
   "source": [
    "# Build graph for feature extraction\n",
    "G = nx.Graph()\n",
    "for _, digimon in digimon_df.iterrows():\n",
    "    G.add_node(\n",
    "        digimon['name_en'],\n",
    "        level=digimon['level'],\n",
    "        type=digimon['type'],\n",
    "        attribute=digimon['attribute']\n",
    "    )\n",
    "\n",
    "# Add evolution edges\n",
    "for evo in evolution_data:\n",
    "    if evo['from_digimon'] in G and evo['to_digimon'] in G:\n",
    "        G.add_edge(evo['from_digimon'], evo['to_digimon'], weight=2.0)\n",
    "\n",
    "# Add shared type edges (sample)\n",
    "type_groups = digimon_df.groupby('type')['name_en'].apply(list)\n",
    "for type_name, digimon_list in type_groups.items():\n",
    "    if 2 <= len(digimon_list) <= 20:\n",
    "        for i in range(len(digimon_list)):\n",
    "            for j in range(i + 1, min(i + 3, len(digimon_list))):\n",
    "                if not G.has_edge(digimon_list[i], digimon_list[j]):\n",
    "                    G.add_edge(digimon_list[i], digimon_list[j], weight=0.5)\n",
    "\n",
    "print(f\"\\nGraph built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None cannot be a node",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m G = nx.Graph()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, digimon \u001b[38;5;129;01min\u001b[39;00m digimon_df.iterrows():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mG\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdigimon\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname_en\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdigimon\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlevel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mdigimon\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdigimon\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattribute\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Add evolution edges\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m evo \u001b[38;5;129;01min\u001b[39;00m evolution_data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projects/project-yggdrasil/.venv/lib/python3.11/site-packages/networkx/classes/graph.py:558\u001b[39m, in \u001b[36mGraph.add_node\u001b[39m\u001b[34m(self, node_for_adding, **attr)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node_for_adding \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._node:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node_for_adding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNone cannot be a node\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    559\u001b[39m     \u001b[38;5;28mself\u001b[39m._adj[node_for_adding] = \u001b[38;5;28mself\u001b[39m.adjlist_inner_dict_factory()\n\u001b[32m    560\u001b[39m     attr_dict = \u001b[38;5;28mself\u001b[39m._node[node_for_adding] = \u001b[38;5;28mself\u001b[39m.node_attr_dict_factory()\n",
      "\u001b[31mValueError\u001b[39m: None cannot be a node"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:03:16.093514Z",
     "start_time": "2025-08-12T02:03:15.238667Z"
    }
   },
   "source": "# Feature Engineering\nprint(\"=== FEATURE ENGINEERING ===\")\n\n# Initialize feature DataFrame\nfeatures_df = digimon_df[['name_en', 'level', 'type', 'attribute']].copy()\nfeatures_df.set_index('name_en', inplace=True)\n\n# 1. Graph-based features\nprint(\"\\nCalculating graph features...\")\n# Import the function from utils\nfrom utils import calculate_centrality_measures\ncentrality_df = calculate_centrality_measures(G)\nfeatures_df = features_df.join(centrality_df)\n\n# 2. Evolution features\nprint(\"Calculating evolution features...\")\nevolution_df = pd.DataFrame(evolution_data)\nin_degree = evolution_df.groupby('to_digimon').size()\nout_degree = evolution_df.groupby('from_digimon').size()\n\nfeatures_df['evolution_in_degree'] = features_df.index.map(in_degree).fillna(0)\nfeatures_df['evolution_out_degree'] = features_df.index.map(out_degree).fillna(0)\nfeatures_df['evolution_total_degree'] = features_df['evolution_in_degree'] + features_df['evolution_out_degree']\n\n# 3. Move-based features\nprint(\"Calculating move features...\")\nmove_counts = moves_df.groupby('digimon')['move'].count()\nunique_moves = moves_df.groupby('digimon')['move'].nunique()\nfeatures_df['total_moves'] = features_df.index.map(move_counts).fillna(0)\nfeatures_df['unique_moves'] = features_df.index.map(unique_moves).fillna(0)\n\n# Calculate move rarity score\nmove_frequency = moves_df['move'].value_counts()\nmove_rarity = {}\nfor digimon in features_df.index:\n    digimon_moves = moves_df[moves_df['digimon'] == digimon]['move'].tolist()\n    if digimon_moves:\n        rarity_scores = [1 / move_frequency[move] for move in digimon_moves]\n        move_rarity[digimon] = np.mean(rarity_scores)\n    else:\n        move_rarity[digimon] = 0\n\nfeatures_df['move_rarity_score'] = features_df.index.map(move_rarity)\n\n# 4. Network neighborhood features\nprint(\"Calculating neighborhood features...\")\nfor node in features_df.index:\n    if node in G:\n        neighbors = list(G.neighbors(node))\n        if neighbors:\n            # Neighbor type diversity\n            neighbor_types = [G.nodes[n].get('type', 'Unknown') for n in neighbors]\n            features_df.loc[node, 'neighbor_type_diversity'] = len(set(neighbor_types))\n            \n            # Neighbor attribute distribution\n            neighbor_attrs = [G.nodes[n].get('attribute', 'Unknown') for n in neighbors]\n            attr_counts = Counter(neighbor_attrs)\n            features_df.loc[node, 'neighbor_vaccine_ratio'] = attr_counts.get('Vaccine', 0) / len(neighbors)\n            features_df.loc[node, 'neighbor_virus_ratio'] = attr_counts.get('Virus', 0) / len(neighbors)\n            features_df.loc[node, 'neighbor_data_ratio'] = attr_counts.get('Data', 0) / len(neighbors)\n        else:\n            features_df.loc[node, 'neighbor_type_diversity'] = 0\n            features_df.loc[node, 'neighbor_vaccine_ratio'] = 0\n            features_df.loc[node, 'neighbor_virus_ratio'] = 0\n            features_df.loc[node, 'neighbor_data_ratio'] = 0\n    else:\n        features_df.loc[node, ['neighbor_type_diversity', 'neighbor_vaccine_ratio', \n                              'neighbor_virus_ratio', 'neighbor_data_ratio']] = 0\n\n# 5. Level-based features (encode as ordinal)\nlevel_order = {'Baby': 0, 'In-Training': 1, 'Rookie': 2, 'Champion': 3, \n               'Ultimate': 4, 'Mega': 5, 'Ultra': 6}\nfeatures_df['level_numeric'] = features_df['level'].map(level_order).fillna(3)  # Default to Champion\n\nprint(f\"\\nTotal features created: {len(features_df.columns)}\")\nprint(\"Feature columns:\", list(features_df.columns))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING ===\n",
      "\n",
      "Calculating graph features...\n",
      "Calculating evolution features...\n",
      "Calculating move features...\n",
      "Calculating neighborhood features...\n",
      "\n",
      "Total features created: 19\n",
      "Feature columns: ['level', 'type', 'attribute', 'degree', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'evolution_in_degree', 'evolution_out_degree', 'evolution_total_degree', 'total_moves', 'unique_moves', 'move_rarity_score', 'neighbor_type_diversity', 'neighbor_vaccine_ratio', 'neighbor_virus_ratio', 'neighbor_data_ratio', 'level_numeric']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:03:17.576248Z",
     "start_time": "2025-08-12T02:03:17.554086Z"
    }
   },
   "source": [
    "# Display feature statistics\n",
    "print(\"\\n=== FEATURE STATISTICS ===\")\n",
    "numeric_features = features_df.select_dtypes(include=[np.number]).columns\n",
    "print(features_df[numeric_features].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE STATISTICS ===\n",
      "       degree  betweenness  closeness  eigenvector     pagerank  \\\n",
      "count  1249.0       1249.0     1249.0  1249.000000  1249.000000   \n",
      "mean      0.0          0.0        0.0     0.028296     0.000801   \n",
      "std       0.0          0.0        0.0     0.000000     0.000000   \n",
      "min       0.0          0.0        0.0     0.028296     0.000801   \n",
      "25%       0.0          0.0        0.0     0.028296     0.000801   \n",
      "50%       0.0          0.0        0.0     0.028296     0.000801   \n",
      "75%       0.0          0.0        0.0     0.028296     0.000801   \n",
      "max       0.0          0.0        0.0     0.028296     0.000801   \n",
      "\n",
      "       evolution_in_degree  evolution_out_degree  evolution_total_degree  \\\n",
      "count          1258.000000           1258.000000             1258.000000   \n",
      "mean              2.951510              2.977742                5.929253   \n",
      "std               2.280986              0.254438                2.322917   \n",
      "min               0.000000              0.000000                0.000000   \n",
      "25%               1.000000              3.000000                4.000000   \n",
      "50%               3.000000              3.000000                6.000000   \n",
      "75%               4.000000              3.000000                7.000000   \n",
      "max              18.000000              3.000000               21.000000   \n",
      "\n",
      "       total_moves  unique_moves  move_rarity_score  neighbor_type_diversity  \\\n",
      "count  1258.000000   1258.000000        1258.000000                   1258.0   \n",
      "mean      1.934022      1.934022           0.858139                      0.0   \n",
      "std       0.931002      0.931002           0.246204                      0.0   \n",
      "min       0.000000      0.000000           0.000000                      0.0   \n",
      "25%       1.000000      1.000000           0.750000                      0.0   \n",
      "50%       2.000000      2.000000           1.000000                      0.0   \n",
      "75%       2.000000      2.000000           1.000000                      0.0   \n",
      "max       8.000000      8.000000           1.000000                      0.0   \n",
      "\n",
      "       neighbor_vaccine_ratio  neighbor_virus_ratio  neighbor_data_ratio  \\\n",
      "count                  1258.0                1258.0               1258.0   \n",
      "mean                      0.0                   0.0                  0.0   \n",
      "std                       0.0                   0.0                  0.0   \n",
      "min                       0.0                   0.0                  0.0   \n",
      "25%                       0.0                   0.0                  0.0   \n",
      "50%                       0.0                   0.0                  0.0   \n",
      "75%                       0.0                   0.0                  0.0   \n",
      "max                       0.0                   0.0                  0.0   \n",
      "\n",
      "       level_numeric  \n",
      "count    1258.000000  \n",
      "mean        3.602544  \n",
      "std         0.996921  \n",
      "min         2.000000  \n",
      "25%         3.000000  \n",
      "50%         3.000000  \n",
      "75%         5.000000  \n",
      "max         5.000000  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Task 1: Predict Type from Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:03:19.086012Z",
     "start_time": "2025-08-12T02:03:19.073055Z"
    }
   },
   "source": [
    "# Prepare data for type prediction\n",
    "print(\"=== TYPE PREDICTION TASK ===\")\n",
    "\n",
    "# Filter to common types (at least 20 examples)\n",
    "type_counts = features_df['type'].value_counts()\n",
    "common_types = type_counts[type_counts >= 20].index\n",
    "type_data = features_df[features_df['type'].isin(common_types)].copy()\n",
    "\n",
    "print(f\"Predicting {len(common_types)} types with {len(type_data)} samples\")\n",
    "print(f\"\\nType distribution:\")\n",
    "print(type_data['type'].value_counts().head(10))\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['degree', 'betweenness', 'closeness', 'eigenvector', 'pagerank',\n",
    "                'evolution_in_degree', 'evolution_out_degree', 'total_moves', \n",
    "                'move_rarity_score', 'neighbor_type_diversity', 'level_numeric',\n",
    "                'neighbor_vaccine_ratio', 'neighbor_virus_ratio', 'neighbor_data_ratio']\n",
    "\n",
    "X_type = type_data[feature_cols].fillna(0)\n",
    "y_type = type_data['type']\n",
    "\n",
    "# Encode target\n",
    "le_type = LabelEncoder()\n",
    "y_type_encoded = le_type.fit_transform(y_type)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_type, y_type_encoded, test_size=0.2, random_state=42, stratify=y_type_encoded\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TYPE PREDICTION TASK ===\n",
      "Predicting 22 types with 782 samples\n",
      "\n",
      "Type distribution:\n",
      "type\n",
      "cyborg Type      98\n",
      "mutation Type    56\n",
      "Puppet Type      48\n",
      "Machine Type     41\n",
      "insect Type      41\n",
      "Demon Type       39\n",
      "beast Type       39\n",
      "Beastman Type    38\n",
      "God Type         38\n",
      "Slime Type       35\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: (625, 14)\n",
      "Test set: (157, 14)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:06:58.136925Z",
     "start_time": "2025-08-12T02:06:37.001933Z"
    }
   },
   "source": "# Train multiple models\nmodels = {\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n}\n\n# Add XGBoost or GradientBoosting\nif HAS_XGBOOST:\n    models['XGBoost'] = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')\nelse:\n    models['Gradient Boosting'] = GradientBoostingClassifier(n_estimators=100, random_state=42)\n\nresults = {}\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    # Use scaled data for neural network and logistic regression\n    if name in ['Neural Network', 'Logistic Regression']:\n        model.fit(X_train_scaled, y_train)\n        y_pred = model.predict(X_test_scaled)\n        # Cross-validation\n        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n    else:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        # Cross-validation\n        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n    \n    # Calculate metrics\n    test_accuracy = (y_pred == y_test).mean()\n    \n    results[name] = {\n        'model': model,\n        'y_pred': y_pred,\n        'test_accuracy': test_accuracy,\n        'cv_mean': cv_scores.mean(),\n        'cv_std': cv_scores.std()\n    }\n    \n    print(f\"  Test Accuracy: {test_accuracy:.3f}\")\n    print(f\"  CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n\n# Baseline accuracy (most common class)\nfrom scipy import stats\nmost_common_class = stats.mode(y_type_encoded, keepdims=True).mode[0]\nbaseline_accuracy = (y_type_encoded == most_common_class).mean()\nprint(f\"\\nBaseline accuracy (most common type): {baseline_accuracy:.3f}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n",
      "  Test Accuracy: 0.121\n",
      "  CV Accuracy: 0.163 (+/- 0.016)\n",
      "\n",
      "Training Neural Network...\n",
      "  Test Accuracy: 0.140\n",
      "  CV Accuracy: 0.142 (+/- 0.021)\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Test Accuracy: 0.210\n",
      "  CV Accuracy: 0.179 (+/- 0.022)\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Test Accuracy: 0.121\n",
      "  CV Accuracy: 0.162 (+/- 0.043)\n",
      "\n",
      "Baseline accuracy (most common type): 0.125\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:14.609846Z",
     "start_time": "2025-08-12T02:07:14.603712Z"
    }
   },
   "source": "# Feature importance for best model\nbest_model_name = max(results, key=lambda x: results[x]['test_accuracy'])\nbest_model = results[best_model_name]['model']\nprint(f\"\\n=== FEATURE IMPORTANCE ({best_model_name}) ===\")\n\nif hasattr(best_model, 'feature_importances_'):\n    importances = best_model.feature_importances_\n    feature_importance_df = pd.DataFrame({\n        'feature': feature_cols,\n        'importance': importances\n    }).sort_values('importance', ascending=False)\n    \n    # Plot feature importance\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.barh(range(len(feature_importance_df)), \n                    feature_importance_df['importance'],\n                    color='skyblue')\n    ax.set_yticks(range(len(feature_importance_df)))\n    ax.set_yticklabels(feature_importance_df['feature'])\n    ax.set_xlabel('Importance')\n    ax.set_title(f'Feature Importance for Type Prediction ({best_model_name})', \n                fontsize=14, fontweight='bold')\n    ax.invert_yaxis()\n    \n    # Add value labels\n    for i, bar in enumerate(bars):\n        width = bar.get_width()\n        ax.text(width, bar.get_y() + bar.get_height()/2, \n                f'{width:.3f}', ha='left', va='center', fontsize=8)\n    \n    plt.tight_layout()\n    save_figure(fig, \"type_prediction_feature_importance\", notebook_name=notebook_name)\n    plt.show()\n    \n    print(\"\\nTop 5 most important features:\")\n    print(feature_importance_df.head())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Task 2: Predict Attribute"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:22.622943Z",
     "start_time": "2025-08-12T02:07:22.609102Z"
    }
   },
   "source": [
    "# Prepare data for attribute prediction\n",
    "print(\"\\n=== ATTRIBUTE PREDICTION TASK ===\")\n",
    "\n",
    "# Filter out Unknown attributes\n",
    "attr_data = features_df[features_df['attribute'].isin(['Vaccine', 'Virus', 'Data', 'Free'])].copy()\n",
    "print(f\"Predicting attributes with {len(attr_data)} samples\")\n",
    "print(f\"\\nAttribute distribution:\")\n",
    "print(attr_data['attribute'].value_counts())\n",
    "\n",
    "# Add type as one-hot encoded feature\n",
    "type_dummies = pd.get_dummies(attr_data['type'], prefix='type')\n",
    "# Keep only top 20 most common types\n",
    "top_types = attr_data['type'].value_counts().head(20).index\n",
    "type_features = [f'type_{t}' for t in top_types if f'type_{t}' in type_dummies.columns]\n",
    "type_dummies = type_dummies[type_features]\n",
    "\n",
    "# Combine features\n",
    "X_attr = pd.concat([attr_data[feature_cols], type_dummies], axis=1).fillna(0)\n",
    "y_attr = attr_data['attribute']\n",
    "\n",
    "# Split data\n",
    "X_train_attr, X_test_attr, y_train_attr, y_test_attr = train_test_split(\n",
    "    X_attr, y_attr, test_size=0.2, random_state=42, stratify=y_attr\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_attr.shape}\")\n",
    "print(f\"Test set: {X_test_attr.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATTRIBUTE PREDICTION TASK ===\n",
      "Predicting attributes with 1086 samples\n",
      "\n",
      "Attribute distribution:\n",
      "attribute\n",
      "Virus      397\n",
      "Data       308\n",
      "Vaccine    295\n",
      "Free        86\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set: (868, 34)\n",
      "Test set: (218, 34)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:25.049415Z",
     "start_time": "2025-08-12T02:07:23.962461Z"
    }
   },
   "source": "# Train model for attribute prediction\nprint(\"\\nTraining model for attribute prediction...\")\n\n# Encode labels\nle_attr = LabelEncoder()\ny_train_attr_encoded = le_attr.fit_transform(y_train_attr)\ny_test_attr_encoded = le_attr.transform(y_test_attr)\n\n# Use XGBoost if available, otherwise GradientBoosting\nif HAS_XGBOOST:\n    print(\"Using XGBoost...\")\n    attr_model = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss')\nelse:\n    print(\"Using GradientBoosting as fallback...\")\n    attr_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n\nattr_model.fit(X_train_attr, y_train_attr_encoded)\n\n# Predictions\ny_pred_attr = attr_model.predict(X_test_attr)\ny_pred_proba = attr_model.predict_proba(X_test_attr)\n\n# Evaluate\nattr_accuracy = (y_pred_attr == y_test_attr_encoded).mean()\nprint(f\"Test Accuracy: {attr_accuracy:.3f}\")\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_attr, le_attr.inverse_transform(y_pred_attr)))\n\n# Confusion matrix\ncm = confusion_matrix(y_test_attr_encoded, y_pred_attr)\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=le_attr.classes_, yticklabels=le_attr.classes_, ax=ax)\nax.set_xlabel('Predicted')\nax.set_ylabel('Actual')\nax.set_title('Attribute Prediction Confusion Matrix', fontsize=14, fontweight='bold')\nplt.tight_layout()\nsave_figure(fig, \"attribute_prediction_confusion_matrix\", notebook_name=notebook_name)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Task 3: Predict Evolution Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:28.013106Z",
     "start_time": "2025-08-12T02:07:28.002165Z"
    }
   },
   "source": [
    "# Create evolution likelihood target\n",
    "print(\"\\n=== EVOLUTION LIKELIHOOD PREDICTION ===\")\n",
    "\n",
    "# Create binary target: has_evolution (either evolves from or to something)\n",
    "features_df['has_evolution'] = (features_df['evolution_total_degree'] > 0).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "evolution_features = ['degree', 'betweenness', 'closeness', 'eigenvector', 'pagerank',\n",
    "                     'total_moves', 'move_rarity_score', 'neighbor_type_diversity', \n",
    "                     'level_numeric', 'neighbor_vaccine_ratio', 'neighbor_virus_ratio']\n",
    "\n",
    "X_evo = features_df[evolution_features].fillna(0)\n",
    "y_evo = features_df['has_evolution']\n",
    "\n",
    "print(f\"Evolution distribution:\")\n",
    "print(y_evo.value_counts())\n",
    "print(f\"\\nPercentage with evolution: {y_evo.mean():.1%}\")\n",
    "\n",
    "# Split data\n",
    "X_train_evo, X_test_evo, y_train_evo, y_test_evo = train_test_split(\n",
    "    X_evo, y_evo, test_size=0.2, random_state=42, stratify=y_evo\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVOLUTION LIKELIHOOD PREDICTION ===\n",
      "Evolution distribution:\n",
      "has_evolution\n",
      "1    1249\n",
      "0       9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage with evolution: 99.3%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:33.910055Z",
     "start_time": "2025-08-12T02:07:32.445794Z"
    }
   },
   "source": "# Train Random Forest for evolution prediction\nprint(\"\\nTraining Random Forest for evolution likelihood...\")\n\nrf_evo = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf_evo.fit(X_train_evo, y_train_evo)\n\n# Predictions\ny_pred_evo = rf_evo.predict(X_test_evo)\ny_pred_proba_evo = rf_evo.predict_proba(X_test_evo)[:, 1]\n\n# Evaluate\nevo_accuracy = (y_pred_evo == y_test_evo).mean()\nprint(f\"Test Accuracy: {evo_accuracy:.3f}\")\n\n# ROC-AUC\nroc_auc = roc_auc_score(y_test_evo, y_pred_proba_evo)\nprint(f\"ROC-AUC Score: {roc_auc:.3f}\")\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test_evo, y_pred_proba_evo)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# ROC Curve\nax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\nax1.set_xlim([0.0, 1.0])\nax1.set_ylim([0.0, 1.05])\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('ROC Curve - Evolution Likelihood', fontsize=14, fontweight='bold')\nax1.legend(loc=\"lower right\")\n\n# Feature Importance\nimportances_evo = rf_evo.feature_importances_\nimportance_df_evo = pd.DataFrame({\n    'feature': evolution_features,\n    'importance': importances_evo\n}).sort_values('importance', ascending=True)\n\nbars = ax2.barh(range(len(importance_df_evo)), importance_df_evo['importance'], color='skyblue')\nax2.set_yticks(range(len(importance_df_evo)))\nax2.set_yticklabels(importance_df_evo['feature'])\nax2.set_xlabel('Importance')\nax2.set_title('Feature Importance - Evolution Likelihood', fontsize=14, fontweight='bold')\n\n# Add value labels\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    ax2.text(width, bar.get_y() + bar.get_height()/2, \n             f'{width:.3f}', ha='left', va='center', fontsize=8)\n\nplt.tight_layout()\nsave_figure(fig, \"evolution_prediction_analysis\", notebook_name=notebook_name)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:40.003570Z",
     "start_time": "2025-08-12T02:07:39.999238Z"
    }
   },
   "source": "# SHAP analysis for type prediction (if available)\nif HAS_SHAP:\n    print(\"=== SHAP ANALYSIS FOR TYPE PREDICTION ===\")\n    \n    # Use a sample for SHAP (computationally expensive)\n    sample_size = min(100, len(X_test))\n    X_sample = X_test.iloc[:sample_size]\n    \n    # Get the best model (Random Forest or XGBoost/GradientBoosting)\n    if 'Random Forest' in results:\n        model_for_shap = results['Random Forest']['model']\n    elif 'XGBoost' in results:\n        model_for_shap = results['XGBoost']['model']\n    else:\n        model_for_shap = results['Gradient Boosting']['model']\n    \n    # Calculate SHAP values\n    explainer = shap.TreeExplainer(model_for_shap)\n    shap_values = explainer.shap_values(X_sample)\n    \n    # For multiclass, take the first class\n    if isinstance(shap_values, list):\n        shap_values_class0 = shap_values[0]\n    else:\n        shap_values_class0 = shap_values\n    \n    # Summary plot\n    plt.figure(figsize=(10, 8))\n    shap.summary_plot(shap_values_class0, X_sample, show=False)\n    plt.title('SHAP Summary Plot - Type Prediction', fontsize=14, fontweight='bold', pad=20)\n    plt.tight_layout()\n    save_figure(plt.gcf(), \"shap_summary_type_prediction\", notebook_name=notebook_name)\n    plt.show()\nelse:\n    print(\"SHAP analysis skipped (SHAP not available)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:42.975400Z",
     "start_time": "2025-08-12T02:07:42.970365Z"
    }
   },
   "source": "# Create performance summary\nprint(\"\\n=== MODEL PERFORMANCE SUMMARY ===\")\n\n# Type prediction results\nprint(\"\\n1. Type Prediction:\")\nprint(f\"   Classes: {len(common_types)}\")\nprint(f\"   Baseline: {baseline_accuracy:.3f}\")\nfor name, result in results.items():\n    print(f\"   {name}: {result['test_accuracy']:.3f} (CV: {result['cv_mean']:.3f} ± {result['cv_std']*2:.3f})\")\n\n# Attribute prediction\nprint(\"\\n2. Attribute Prediction:\")\nprint(f\"   Classes: 4 (Vaccine, Virus, Data, Free)\")\nmodel_name = \"XGBoost\" if HAS_XGBOOST else \"Gradient Boosting\"\nprint(f\"   {model_name} Accuracy: {attr_accuracy:.3f}\")\n\n# Evolution prediction\nprint(\"\\n3. Evolution Likelihood:\")\nprint(f\"   Positive class ratio: {y_evo.mean():.1%}\")\nprint(f\"   Random Forest Accuracy: {evo_accuracy:.3f}\")\nprint(f\"   ROC-AUC: {roc_auc:.3f}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL PERFORMANCE SUMMARY ===\n",
      "\n",
      "1. Type Prediction:\n",
      "   Classes: 22\n",
      "   Baseline: 0.125\n",
      "   Random Forest: 0.121 (CV: 0.163 ± 0.016)\n",
      "   Neural Network: 0.140 (CV: 0.142 ± 0.021)\n",
      "   Logistic Regression: 0.210 (CV: 0.179 ± 0.022)\n",
      "   Gradient Boosting: 0.121 (CV: 0.162 ± 0.043)\n",
      "\n",
      "2. Attribute Prediction:\n",
      "   Classes: 4 (Vaccine, Virus, Data, Free)\n",
      "   Gradient Boosting Accuracy: 0.372\n",
      "\n",
      "3. Evolution Likelihood:\n",
      "   Positive class ratio: 99.3%\n",
      "   Random Forest Accuracy: 1.000\n",
      "   ROC-AUC: 1.000\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:44.995426Z",
     "start_time": "2025-08-12T02:07:44.164543Z"
    }
   },
   "source": "# Visualize model comparison\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Prepare data for visualization\nmodel_names = list(results.keys())\naccuracies = [results[name]['test_accuracy'] for name in model_names]\ncv_means = [results[name]['cv_mean'] for name in model_names]\ncv_stds = [results[name]['cv_std'] for name in model_names]\n\nx = np.arange(len(model_names))\nwidth = 0.35\n\n# Plot bars\nbars1 = ax.bar(x - width/2, accuracies, width, label='Test Accuracy', color='skyblue')\nbars2 = ax.bar(x + width/2, cv_means, width, label='CV Mean', color='lightcoral', yerr=cv_stds)\n\n# Add baseline line\nax.axhline(y=baseline_accuracy, color='red', linestyle='--', alpha=0.5, label='Baseline')\n\n# Customize\nax.set_xlabel('Model')\nax.set_ylabel('Accuracy')\nax.set_title('Model Performance Comparison - Type Prediction', fontsize=16, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(model_names, rotation=45, ha='right')\nax.legend()\nax.set_ylim(0, 1)\n\n# Add value labels\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nsave_figure(fig, \"model_performance_comparison\", notebook_name=notebook_name)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:50.050354Z",
     "start_time": "2025-08-12T02:07:50.024866Z"
    }
   },
   "source": "# Export results\nfrom pathlib import Path\nimport pickle\nimport json\n\nresults_dir = Path(f'../results/{notebook_name}/data')\nmodels_dir = Path(f'../results/{notebook_name}/models')\nresults_dir.mkdir(parents=True, exist_ok=True)\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\n# Save best models\nbest_type_model = results[best_model_name]['model']\nwith open(models_dir / 'type_predictor.pkl', 'wb') as f:\n    pickle.dump({\n        'model': best_type_model,\n        'scaler': scaler if best_model_name in ['Neural Network', 'Logistic Regression'] else None,\n        'label_encoder': le_type,\n        'feature_cols': feature_cols\n    }, f)\n\nwith open(models_dir / 'attribute_predictor.pkl', 'wb') as f:\n    pickle.dump({\n        'model': attr_model,\n        'label_encoder': le_attr,\n        'feature_cols': list(X_attr.columns)\n    }, f)\n\nwith open(models_dir / 'evolution_predictor.pkl', 'wb') as f:\n    pickle.dump({\n        'model': rf_evo,\n        'feature_cols': evolution_features\n    }, f)\n\n# Save performance metrics\nmodel_type = 'XGBoost' if HAS_XGBOOST else 'Gradient Boosting'\nml_results = {\n    'type_prediction': {\n        'num_classes': len(common_types),\n        'baseline_accuracy': float(baseline_accuracy),\n        'models': {\n            name: {\n                'test_accuracy': float(result['test_accuracy']),\n                'cv_mean': float(result['cv_mean']),\n                'cv_std': float(result['cv_std'])\n            } for name, result in results.items()\n        },\n        'best_model': best_model_name\n    },\n    'attribute_prediction': {\n        'num_classes': 4,\n        'test_accuracy': float(attr_accuracy),\n        'model': model_type\n    },\n    'evolution_prediction': {\n        'positive_ratio': float(y_evo.mean()),\n        'test_accuracy': float(evo_accuracy),\n        'roc_auc': float(roc_auc),\n        'model': 'Random Forest'\n    }\n}\n\nwith open(results_dir / 'ml_results.json', 'w') as f:\n    json.dump(ml_results, f, indent=2)\n\n# Save feature importance\nif 'feature_importance_df' in locals():\n    feature_importance_df.to_csv(results_dir / 'feature_importance_type_prediction.csv', index=False)\n\nimportance_df_evo.to_csv(results_dir / 'feature_importance_evolution_prediction.csv', index=False)\n\nprint(\"Machine learning results exported successfully!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Findings\n",
    "\n",
    "### Predictive Modeling Insights:\n",
    "\n",
    "1. **Type Prediction**:\n",
    "   - Graph features (centrality measures) are strong predictors of Digimon type\n",
    "   - Models significantly outperform baseline, showing learnable patterns\n",
    "   - Neighbor characteristics and evolution degree are important features\n",
    "\n",
    "2. **Attribute Prediction**:\n",
    "   - Type information significantly improves attribute prediction\n",
    "   - Network neighborhood features reveal attribute clustering\n",
    "   - Good prediction accuracy suggests systematic type-attribute relationships\n",
    "\n",
    "3. **Evolution Likelihood**:\n",
    "   - Level and centrality measures strongly predict evolution potential\n",
    "   - Move diversity and rarity correlate with evolution\n",
    "   - High ROC-AUC indicates reliable evolution predictions\n",
    "\n",
    "4. **Feature Importance**:\n",
    "   - Graph centrality measures consistently important across tasks\n",
    "   - Evolution relationships create predictable patterns\n",
    "   - Move characteristics provide unique predictive value\n",
    "\n",
    "These models can be used for game design insights, predicting missing data, and understanding the underlying structure of the Digimon universe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:07:51.831289Z",
     "start_time": "2025-08-12T02:07:51.827681Z"
    }
   },
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"Predictive modeling analysis complete! Database connection closed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive modeling analysis complete! Database connection closed.\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}