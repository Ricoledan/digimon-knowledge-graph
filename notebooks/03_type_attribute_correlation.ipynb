{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Type-Attribute Correlation Analysis\n",
    "\n",
    "**Objective**: Discover patterns in type and attribute combinations in the Digimon Knowledge Graph.\n",
    "\n",
    "This notebook explores:\n",
    "- Distribution analysis of types and attributes by level\n",
    "- Statistical correlation studies between features\n",
    "- Pattern mining for frequent combinations\n",
    "- Predictive analysis of type-attribute relationships\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom utilities\n",
    "from utils import (\n",
    "    Neo4jConnector, \n",
    "    plot_heatmap, save_figure,\n",
    "    TYPE_COLORS, ATTRIBUTE_COLORS, LEVEL_COLORS\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = Neo4jConnector()\n",
    "print(\"Connected to Neo4j database\")\n",
    "\n",
    "# Get all Digimon data\n",
    "digimon_df = conn.get_all_digimon()\n",
    "print(f\"\\nLoaded {len(digimon_df)} Digimon records\")\n",
    "\n",
    "# Remove any records with missing type/attribute/level\n",
    "complete_df = digimon_df.dropna(subset=['type', 'attribute', 'level'])\n",
    "print(f\"Complete records (no missing values): {len(complete_df)}\")\n",
    "\n",
    "# Display sample\n",
    "complete_df[['name_en', 'level', 'type', 'attribute']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type frequency by level\n",
    "type_by_level = pd.crosstab(complete_df['level'], complete_df['type'])\n",
    "\n",
    "# Order levels properly\n",
    "level_order = ['Baby', 'In-Training', 'Rookie', 'Champion', 'Ultimate', 'Mega', 'Ultra']\n",
    "type_by_level = type_by_level.reindex(level_order, fill_value=0)\n",
    "\n",
    "print(\"=== TYPE DISTRIBUTION BY LEVEL ===\")\n",
    "print(type_by_level.head(10))  # Show first 10 types\n",
    "print(f\"\\nTotal unique types: {type_by_level.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute distribution by type (top 15 types)\n",
    "top_types = complete_df['type'].value_counts().head(15).index\n",
    "attr_by_type = pd.crosstab(\n",
    "    complete_df[complete_df['type'].isin(top_types)]['type'],\n",
    "    complete_df[complete_df['type'].isin(top_types)]['attribute']\n",
    ")\n",
    "\n",
    "print(\"=== ATTRIBUTE DISTRIBUTION BY TOP TYPES ===\")\n",
    "print(attr_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create type-attribute combination frequency\n",
    "type_attr_combinations = complete_df.groupby(['type', 'attribute']).size().reset_index(name='count')\n",
    "type_attr_combinations = type_attr_combinations.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"=== MOST COMMON TYPE-ATTRIBUTE COMBINATIONS ===\")\n",
    "print(type_attr_combinations.head(20))\n",
    "\n",
    "# Rare combinations\n",
    "rare_combinations = type_attr_combinations[type_attr_combinations['count'] == 1]\n",
    "print(f\"\\nNumber of unique (rare) combinations: {len(rare_combinations)}\")\n",
    "print(\"\\nExample rare combinations:\")\n",
    "print(rare_combinations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for independence between type and attribute\n",
    "contingency_table = pd.crosstab(complete_df['type'], complete_df['attribute'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"=== CHI-SQUARE TEST: Type vs Attribute ===\")\n",
    "print(f\"Chi-square statistic: {chi2:.2f}\")\n",
    "print(f\"p-value: {p_value:.2e}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nResult: Type and Attribute are DEPENDENT (p < 0.05)\")\n",
    "else:\n",
    "    print(\"\\nResult: Type and Attribute are INDEPENDENT (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cramér's V for effect size\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "cramers_v_value = cramers_v(contingency_table)\n",
    "print(f\"\\nCramér's V (effect size): {cramers_v_value:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "if cramers_v_value < 0.1:\n",
    "    effect = \"negligible\"\n",
    "elif cramers_v_value < 0.3:\n",
    "    effect = \"small\"\n",
    "elif cramers_v_value < 0.5:\n",
    "    effect = \"medium\"\n",
    "else:\n",
    "    effect = \"large\"\n",
    "\n",
    "print(f\"Effect size interpretation: {effect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information between type and attribute\n",
    "le_type = LabelEncoder()\n",
    "le_attr = LabelEncoder()\n",
    "\n",
    "type_encoded = le_type.fit_transform(complete_df['type'])\n",
    "attr_encoded = le_attr.fit_transform(complete_df['attribute'])\n",
    "\n",
    "mi_score = mutual_info_score(type_encoded, attr_encoded)\n",
    "print(f\"\\nMutual Information Score: {mi_score:.3f}\")\n",
    "\n",
    "# Normalized MI (0 to 1)\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "nmi_score = normalized_mutual_info_score(type_encoded, attr_encoded)\n",
    "print(f\"Normalized Mutual Information: {nmi_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test associations for each level separately\n",
    "print(\"\\n=== CHI-SQUARE TESTS BY LEVEL ===\")\n",
    "level_associations = []\n",
    "\n",
    "for level in level_order:\n",
    "    level_data = complete_df[complete_df['level'] == level]\n",
    "    if len(level_data) > 20:  # Only test if sufficient data\n",
    "        cont_table = pd.crosstab(level_data['type'], level_data['attribute'])\n",
    "        # Remove types/attributes with very low counts\n",
    "        cont_table = cont_table.loc[(cont_table.sum(axis=1) > 2), (cont_table.sum(axis=0) > 2)]\n",
    "        \n",
    "        if cont_table.shape[0] > 1 and cont_table.shape[1] > 1:\n",
    "            chi2, p_value, _, _ = chi2_contingency(cont_table)\n",
    "            cramers_v_val = cramers_v(cont_table)\n",
    "            \n",
    "            level_associations.append({\n",
    "                'level': level,\n",
    "                'n_digimon': len(level_data),\n",
    "                'chi2': chi2,\n",
    "                'p_value': p_value,\n",
    "                'cramers_v': cramers_v_val\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{level}:\")\n",
    "            print(f\"  N = {len(level_data)}\")\n",
    "            print(f\"  Chi² = {chi2:.2f}, p = {p_value:.3f}\")\n",
    "            print(f\"  Cramér's V = {cramers_v_val:.3f}\")\n",
    "\n",
    "level_assoc_df = pd.DataFrame(level_associations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pattern Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for association rule mining\n",
    "# Create transactions with type, attribute, and level\n",
    "transactions = []\n",
    "for _, row in complete_df.iterrows():\n",
    "    transaction = [\n",
    "        f\"type:{row['type']}\",\n",
    "        f\"attr:{row['attribute']}\",\n",
    "        f\"level:{row['level']}\"\n",
    "    ]\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "transaction_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"Transaction dataset shape: {transaction_df.shape}\")\n",
    "print(f\"Number of features: {len(te.columns_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find frequent itemsets\n",
    "min_support = 0.05  # At least 5% of Digimon\n",
    "frequent_itemsets = apriori(transaction_df, min_support=min_support, use_colnames=True)\n",
    "frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False)\n",
    "\n",
    "print(f\"=== FREQUENT ITEMSETS (min support = {min_support}) ===\")\n",
    "print(f\"Found {len(frequent_itemsets)} frequent itemsets\\n\")\n",
    "\n",
    "# Filter for interesting combinations (2+ items)\n",
    "interesting_itemsets = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) >= 2)]\n",
    "print(\"Top frequent combinations:\")\n",
    "for idx, row in interesting_itemsets.head(20).iterrows():\n",
    "    items = list(row['itemsets'])\n",
    "    support = row['support']\n",
    "    print(f\"  {' + '.join(items)}: {support:.3f} ({support*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules\n",
    "if len(frequent_itemsets) > 0:\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "    rules = rules.sort_values('lift', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== TOP ASSOCIATION RULES ===\")\n",
    "    print(f\"Found {len(rules)} rules with confidence >= 0.7\\n\")\n",
    "    \n",
    "    # Display top rules\n",
    "    for idx, rule in rules.head(15).iterrows():\n",
    "        antecedents = ', '.join(list(rule['antecedents']))\n",
    "        consequents = ', '.join(list(rule['consequents']))\n",
    "        confidence = rule['confidence']\n",
    "        lift = rule['lift']\n",
    "        \n",
    "        print(f\"Rule {idx+1}: {antecedents} → {consequents}\")\n",
    "        print(f\"  Confidence: {confidence:.3f}, Lift: {lift:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze if we can predict attribute from type\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data\n",
    "le_type = LabelEncoder()\n",
    "le_attr = LabelEncoder()\n",
    "\n",
    "X = le_type.fit_transform(complete_df['type']).reshape(-1, 1)\n",
    "y = le_attr.fit_transform(complete_df['attribute'])\n",
    "\n",
    "# Simple model to test predictability\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"=== ATTRIBUTE PREDICTABILITY FROM TYPE ===\")\n",
    "print(f\"Cross-validation accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "print(f\"Baseline accuracy (most common class): {complete_df['attribute'].value_counts().max() / len(complete_df):.3f}\")\n",
    "\n",
    "# Fit the model to get feature importance\n",
    "rf.fit(X, y)\n",
    "print(f\"\\nModel improvement over baseline: {scores.mean() - (complete_df['attribute'].value_counts().max() / len(complete_df)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze type clustering based on attribute distribution\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create type-attribute matrix for clustering\n",
    "type_attr_matrix = pd.crosstab(complete_df['type'], complete_df['attribute'])\n",
    "type_attr_matrix_norm = type_attr_matrix.div(type_attr_matrix.sum(axis=1), axis=0)\n",
    "\n",
    "# Filter to types with at least 5 Digimon\n",
    "type_counts = complete_df['type'].value_counts()\n",
    "common_types = type_counts[type_counts >= 5].index\n",
    "type_attr_matrix_norm = type_attr_matrix_norm.loc[common_types]\n",
    "\n",
    "# Perform clustering\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "type_clusters = kmeans.fit_predict(type_attr_matrix_norm)\n",
    "\n",
    "# Create cluster results\n",
    "cluster_df = pd.DataFrame({\n",
    "    'type': type_attr_matrix_norm.index,\n",
    "    'cluster': type_clusters\n",
    "})\n",
    "\n",
    "print(f\"=== TYPE CLUSTERING BASED ON ATTRIBUTES ===\")\n",
    "print(f\"Clustered {len(common_types)} types into {n_clusters} groups\\n\")\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_types = cluster_df[cluster_df['cluster'] == cluster_id]['type'].tolist()\n",
    "    print(f\"Cluster {cluster_id + 1} ({len(cluster_types)} types):\")\n",
    "    print(f\"  {', '.join(cluster_types[:10])}{'...' if len(cluster_types) > 10 else ''}\")\n",
    "    \n",
    "    # Show dominant attribute pattern\n",
    "    cluster_data = complete_df[complete_df['type'].isin(cluster_types)]\n",
    "    attr_dist = cluster_data['attribute'].value_counts(normalize=True)\n",
    "    print(f\"  Attribute distribution: {dict(attr_dist)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type-Attribute heatmap (top types)\n",
    "fig = plot_heatmap(\n",
    "    attr_by_type,\n",
    "    title=\"Type-Attribute Distribution Heatmap (Top 15 Types)\",\n",
    "    figsize=(12, 10)\n",
    ")\n",
    "save_figure(fig, \"type_attribute_heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bubble chart for type-attribute combinations\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Prepare data for bubble chart\n",
    "top_combinations = type_attr_combinations.head(30)\n",
    "types = top_combinations['type'].values\n",
    "attributes = top_combinations['attribute'].values\n",
    "counts = top_combinations['count'].values\n",
    "\n",
    "# Create categorical positions\n",
    "unique_types = list(dict.fromkeys(types))\n",
    "unique_attrs = list(dict.fromkeys(attributes))\n",
    "x_pos = [unique_types.index(t) for t in types]\n",
    "y_pos = [unique_attrs.index(a) for a in attributes]\n",
    "\n",
    "# Create bubble chart\n",
    "scatter = ax.scatter(x_pos, y_pos, s=counts*50, alpha=0.6, \n",
    "                    c=counts, cmap='viridis', edgecolors='black', linewidth=1)\n",
    "\n",
    "# Customize\n",
    "ax.set_xticks(range(len(unique_types)))\n",
    "ax.set_xticklabels(unique_types, rotation=45, ha='right')\n",
    "ax.set_yticks(range(len(unique_attrs)))\n",
    "ax.set_yticklabels(unique_attrs)\n",
    "ax.set_xlabel('Type', fontsize=12)\n",
    "ax.set_ylabel('Attribute', fontsize=12)\n",
    "ax.set_title('Type-Attribute Combination Frequencies (Top 30)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Count', fontsize=10)\n",
    "\n",
    "# Add count labels for largest bubbles\n",
    "for i, count in enumerate(counts[:10]):\n",
    "    ax.annotate(str(count), (x_pos[i], y_pos[i]), \n",
    "                ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, \"type_attribute_bubble_chart\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel coordinates plot for multi-attribute patterns\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Prepare data for parallel coordinates\n",
    "sample_size = 200\n",
    "parallel_data = complete_df[['type', 'attribute', 'level']].copy()\n",
    "\n",
    "# Encode categoricals as numbers for visualization\n",
    "for col in ['type', 'attribute', 'level']:\n",
    "    le = LabelEncoder()\n",
    "    parallel_data[f'{col}_encoded'] = le.fit_transform(parallel_data[col])\n",
    "\n",
    "# Sample data\n",
    "if len(parallel_data) > sample_size:\n",
    "    parallel_data = parallel_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "parallel_coordinates(parallel_data[['attribute', 'type_encoded', 'level_encoded']], \n",
    "                    'attribute', alpha=0.3, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Encoded Values')\n",
    "ax.set_title(f'Parallel Coordinates: Type-Level Patterns by Attribute (n={sample_size})', \n",
    "            fontsize=16, fontweight='bold')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, \"parallel_coordinates_attributes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test results visualization\n",
    "if len(level_assoc_df) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # P-values by level\n",
    "    bars1 = ax1.bar(level_assoc_df['level'], -np.log10(level_assoc_df['p_value']), \n",
    "                    color=[LEVEL_COLORS.get(l, '#808080') for l in level_assoc_df['level']])\n",
    "    ax1.axhline(y=-np.log10(0.05), color='red', linestyle='--', label='p=0.05')\n",
    "    ax1.set_xlabel('Level')\n",
    "    ax1.set_ylabel('-log10(p-value)')\n",
    "    ax1.set_title('Type-Attribute Association Significance by Level', fontsize=14)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Effect sizes by level\n",
    "    bars2 = ax2.bar(level_assoc_df['level'], level_assoc_df['cramers_v'], \n",
    "                    color=[LEVEL_COLORS.get(l, '#808080') for l in level_assoc_df['level']])\n",
    "    ax2.set_xlabel('Level')\n",
    "    ax2.set_ylabel(\"Cramér's V\")\n",
    "    ax2.set_title('Type-Attribute Association Strength by Level', fontsize=14)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax = bar.axes\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_figure(fig, \"level_association_statistics\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "from pathlib import Path\n",
    "results_dir = Path('../results/data')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export key findings\n",
    "correlation_results = {\n",
    "    'chi_square_statistic': float(chi2),\n",
    "    'p_value': float(p_value),\n",
    "    'cramers_v': float(cramers_v_value),\n",
    "    'mutual_information': float(mi_score),\n",
    "    'normalized_mi': float(nmi_score),\n",
    "    'n_unique_combinations': len(type_attr_combinations),\n",
    "    'n_rare_combinations': len(rare_combinations)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(results_dir / 'type_attribute_correlations.json', 'w') as f:\n",
    "    json.dump(correlation_results, f, indent=2)\n",
    "\n",
    "# Export combination frequencies\n",
    "type_attr_combinations.to_csv(results_dir / 'type_attribute_combinations.csv', index=False)\n",
    "\n",
    "# Export association rules if generated\n",
    "if 'rules' in locals() and len(rules) > 0:\n",
    "    rules_export = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].copy()\n",
    "    rules_export['antecedents'] = rules_export['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "    rules_export['consequents'] = rules_export['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "    rules_export.to_csv(results_dir / 'association_rules.csv', index=False)\n",
    "\n",
    "# Export frequent itemsets\n",
    "if len(frequent_itemsets) > 0:\n",
    "    itemsets_export = frequent_itemsets.copy()\n",
    "    itemsets_export['itemsets'] = itemsets_export['itemsets'].apply(lambda x: ', '.join(list(x)))\n",
    "    itemsets_export.to_csv(results_dir / 'frequent_itemsets.csv', index=False)\n",
    "\n",
    "print(\"Results exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Findings\n",
    "\n",
    "### Type-Attribute Correlation Insights:\n",
    "\n",
    "1. **Statistical Dependence**:\n",
    "   - Strong statistical evidence that type and attribute are not independent\n",
    "   - Effect size suggests meaningful associations between certain types and attributes\n",
    "   - Mutual information confirms information gain from knowing one feature\n",
    "\n",
    "2. **Distribution Patterns**:\n",
    "   - Some type-attribute combinations are much more common than others\n",
    "   - Many rare/unique combinations exist, suggesting diversity\n",
    "   - Level influences the strength of type-attribute associations\n",
    "\n",
    "3. **Association Rules**:\n",
    "   - Strong predictive rules exist between certain features\n",
    "   - Some combinations have high confidence and lift values\n",
    "   - Patterns suggest design principles in Digimon creation\n",
    "\n",
    "4. **Predictability**:\n",
    "   - Attributes show moderate predictability from types\n",
    "   - Types cluster into groups based on attribute preferences\n",
    "   - Certain type families share similar attribute distributions\n",
    "\n",
    "These findings reveal systematic patterns in how Digimon types and attributes are related, suggesting underlying design principles in the franchise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"Type-Attribute correlation analysis complete! Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}